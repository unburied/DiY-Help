{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiY-WebScrapeHelpSection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOj6GqDU8BPGaY+GKDmnrIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unburied/DiY-Help/blob/master/DiY_WebScrapeHelpSection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf5uohDHRPP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import urlopen as uReq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qv_CQu6Rb5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diy_url = 'https://support.discmakers.com/hc/en-us'\n",
        "\n",
        "# open connection and grab page\n",
        "uClient = uReq(diy_url)\n",
        "page_html = uClient.read()\n",
        "uClient.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd7mMZ1gSepT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get and parse the page\n",
        "diy_soup = soup(page_html, 'html.parser')\n",
        "\n",
        "# added to extracted urls for validity\n",
        "PREPEND = 'https://support.discmakers.com' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv6UAIiqji2P",
        "colab_type": "code",
        "outputId": "3f13041d-68be-4129-b2a1-7af06c414c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data = {} # store main page data\n",
        "\n",
        "# Get blocks containing category urls\n",
        "blocks = diy_soup.findAll('li',{'class':'blocks-item'})\n",
        "\n",
        "for block in blocks:\n",
        "  title = block.findAll('h4', {'class':\"blocks-item-title\"})[0].text\n",
        "  link = block.a.get('href')\n",
        "  full_link = PREPEND + link\n",
        "\n",
        "  data[title] = full_link\n",
        "\n",
        "del data['Contact Us']# removed as it does not contain articles\n",
        "print(f'There are {len(data)} categories containing articles on this page') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 10 categories containing articles on this page\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZMJ-xGsxnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_data = {} # for subpages\n",
        "for k,v in data.items():\n",
        "  subpage_url = v #get url from dict\n",
        "\n",
        "  # create connection to webpage\n",
        "  uClient = uReq(subpage_url)\n",
        "  subpage_html = uClient.read()\n",
        "  uClient.close()\n",
        "\n",
        "  # parse data\n",
        "  subpage_soup = soup(subpage_html, 'html.parser')\n",
        "\n",
        "  # add url on subpages to new dict\n",
        "  sub_sections = subpage_soup.findAll('h3',{'class':'section-h3'})\n",
        "  sub_data[k] = [PREPEND + section.a.get('href') for section in sub_sections]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7fAcHntx9L",
        "colab_type": "code",
        "outputId": "4a50f227-5e41-4f91-dda0-a14d092c8166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "count = sum([len(v) for v in sub_data.values()])\n",
        "print(f'There are {count} sub-categories in this help section')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 40 sub-categories in this help section\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgsTl88ytyjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subsub_data = {} # final subpage before access to articles\n",
        "for k,v in sub_data.items():\n",
        "  subsub_sections = []\n",
        "  for url in v: # need to loop through values list for urls\n",
        "    subsub_url = url\n",
        "\n",
        "    # connect to webpage\n",
        "    uClient = uReq(subsub_url)\n",
        "    subsub_html = uClient.read()\n",
        "    uClient.close()\n",
        "\n",
        "    # parse\n",
        "    subsub_soup = soup(subsub_html, 'html.parser')\n",
        "\n",
        "    # get links\n",
        "    subsub_sections.extend(subsub_soup.findAll('li',\n",
        "                                {'class':'article-list-item'}))\n",
        "    \n",
        "    # check for promoted articles that have alternate class\n",
        "    promoted = subsub_soup.findAll('li',\n",
        "                                {'class':'article-list-item article-promoted'})\n",
        "    if promoted != None: \n",
        "      subsub_sections.extend(promoted)\n",
        "  \n",
        "  # load links into dict using same keys\n",
        "  subsub_data[k]= ([PREPEND + section.a.get('href')\n",
        "                        for section in subsub_sections])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sghQjzNU56Pd",
        "colab_type": "code",
        "outputId": "3a42a744-9a40-4188-d234-7b61a49d567a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "count = sum([len(v) for v in subsub_data.values()])\n",
        "print(f'There are {count} articles in this help section')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 459 articles in this help section\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jARoCh5vBXyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles = {} # to contain all article content\n",
        "urls = {} # used to join data on urls \n",
        "#TODO- add dict that contians url as keys and article title as values\n",
        "\n",
        "for categories in subsub_data.values():\n",
        "  for url in categories:\n",
        "    article_url = url\n",
        "\n",
        "    # connect to webpage\n",
        "    uClient = uReq(article_url)\n",
        "    article_html = uClient.read()\n",
        "    uClient.close()\n",
        "\n",
        "    article_soup = soup(article_html, 'html.parser')\n",
        "\n",
        "    article_title = article_soup.findAll('h1',\n",
        "                                {'class':'article-title'})[0].text.strip()\n",
        "    article_body = article_soup.findAll('div', \n",
        "                                {'class':'article-body'})\n",
        "    article_body = article_body[0].findAll('p')\n",
        "    article_text= [line.text.strip() for line in article_body if line != None]\n",
        "\n",
        "    articles[article_title] = \" \".join(article_text)\n",
        "    urls[article_title] = url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySbwOBtCAFf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def dict2df(data:dict, columns:list, modifier = False): \n",
        "  \"\"\"Prep dicts for dataframe insertion\"\"\"\n",
        "  keys = []\n",
        "  values = []\n",
        "  for k,v in data.items():\n",
        "\n",
        "    if modifier: #for subsub dict containing categories allowing for tidy insert\n",
        "      keys.extend([k for _ in range(len(v))])\n",
        "      values.extend(v)\n",
        "    else:\n",
        "      keys.append(k)\n",
        "      values.append(v)\n",
        "\n",
        "  assert len(keys) == len(values)\n",
        "  keyval = zip(keys, values)\n",
        "\n",
        "  return pd.DataFrame(data=keyval, columns=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39H8ngbYDs4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all dicts into dataframe and assign column names\n",
        "df_subdata = dict2df(subsub_data, columns=['categories', 'urls'], modifier=True)\n",
        "df_articles = dict2df(articles, columns = ['title', 'content'])\n",
        "df_urls = dict2df(urls, columns=['title', 'urls'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35qnYczM7NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge dfs into one\n",
        "diy_df = pd.merge(df_articles, df_urls)\n",
        "diy_df = pd.merge(diy_df, df_subdata.drop_duplicates(), how='left')\n",
        "\n",
        "# subdata had duplicates that didnt align until after merge\n",
        "assert diy_df.shape[0] == df_articles.shape[0] == df_urls.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z20bMOZCaSma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import date\n",
        "\n",
        "today = date.today()\n",
        "today = today.strftime(\"%B %d, %Y\")\n",
        "\n",
        "file_name = 'Help Section Articles-' + today + '.csv'\n",
        "\n",
        "diy_df.to_csv(file_name, index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XgDV6Z7rRW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}